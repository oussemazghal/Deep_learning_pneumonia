{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Comparison of 5 CNN Models for Pneumonia Classification\n",
                "## Comparative Analysis with Visualizations (Ensemble Results)\n",
                "\n",
                "This notebook compares the performance of 5 different deep learning models:\n",
                "1. **CNN Base** - VGG-like CNN trained from scratch\n",
                "2. **VGG16** - Transfer learning with progressive fine-tuning\n",
                "3. **ResNet50** - Transfer learning with skip connections\n",
                "4. **EfficientNet-B0** - Feature extraction only\n",
                "5. **DenseNet121** - Transfer learning with dense connections\n",
                "\n",
                "All models use **ensemble predictions** (average of 3-fold CV models)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json, os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from pathlib import Path\n",
                "\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "plt.rcParams['font.size'] = 11\n",
                "print('Libraries loaded!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Results from All Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_model_result(filepath, model_name):\n",
                "    try:\n",
                "        with open(filepath, 'r') as f:\n",
                "            data = json.load(f)\n",
                "        cv = data.get('cv', data.get('cv_results', {}))\n",
                "        test = data.get('test_ensemble', data.get('test_results_ensemble', {}))\n",
                "        return {\n",
                "            'Model': model_name,\n",
                "            'CV Accuracy': cv.get('accuracy_mean', 0),\n",
                "            'CV AUC': cv.get('auc_mean', 0),\n",
                "            'CV F1': cv.get('f1_mean', 0),\n",
                "            'Test Accuracy': test.get('accuracy', 0),\n",
                "            'Test AUC': test.get('auc', 0),\n",
                "            'Test F1': test.get('f1', 0),\n",
                "            'Test Sensitivity': test.get('sensitivity', test.get('recall', 0)),\n",
                "            'Test Specificity': test.get('specificity', 0),\n",
                "            'Test Precision': test.get('precision', 0),\n",
                "            'Training Time (min)': data.get('total_time_min', data.get('total_training_time_minutes', 0))\n",
                "        }\n",
                "    except FileNotFoundError:\n",
                "        print(f'WARNING: {filepath} not found!')\n",
                "        return None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "models = [\n",
                "    ('model_1_cnn_ensemble_results.json', 'CNN Base'),\n",
                "    ('model_2_vgg16_ensemble_results.json', 'VGG16'),\n",
                "    ('model_3_resnet50_ensemble_results.json', 'ResNet50'),\n",
                "    ('model_4_efficientnet_ensemble_results.json', 'EfficientNet-B0'),\n",
                "    ('model_5_densenet121_ensemble_results.json', 'DenseNet121')\n",
                "]\n",
                "results = []\n",
                "for filepath, name in models:\n",
                "    result = load_model_result(filepath, name)\n",
                "    if result:\n",
                "        results.append(result)\n",
                "        print(f'Loaded: {name}')\n",
                "df = pd.DataFrame(results)\n",
                "print(f'\\nLoaded {len(df)} models')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('\\n' + '='*100)\n",
                "print('MODEL COMPARISON SUMMARY (ENSEMBLE PREDICTIONS)')\n",
                "print('='*100)\n",
                "display_cols = ['Model', 'Test Accuracy', 'Test AUC', 'Test F1', 'Test Sensitivity', 'Test Specificity', 'Training Time (min)']\n",
                "print(df[display_cols].to_string(index=False))\n",
                "print('='*100)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Test Set Performance Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
                "metrics = ['Test Accuracy', 'Test AUC', 'Test F1']\n",
                "colors = ['#2ecc71', '#3498db', '#9b59b6']\n",
                "for ax, metric, color in zip(axes, metrics, colors):\n",
                "    values = df[metric].values\n",
                "    x_pos = np.arange(len(df['Model']))\n",
                "    bars = ax.bar(x_pos, values, color=color, edgecolor='black', alpha=0.8)\n",
                "    ax.set_ylabel(metric, fontweight='bold')\n",
                "    ax.set_ylim(0.7, 1.0)\n",
                "    ax.set_xticks(x_pos)\n",
                "    ax.set_xticklabels(df['Model'], rotation=45, ha='right')\n",
                "    for bar, val in zip(bars, values):\n",
                "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{val:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
                "    ax.grid(axis='y', alpha=0.3)\n",
                "plt.suptitle('Test Set Performance Comparison (Ensemble)', fontsize=16, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig('comparison_test_metrics.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "width = 0.35\n",
                "x = np.arange(len(df['Model']))\n",
                "bars1 = ax.bar(x - width/2, df['Test Sensitivity'], width, label='Sensitivity (Pneumonia)', color='#e74c3c', edgecolor='black')\n",
                "bars2 = ax.bar(x + width/2, df['Test Specificity'], width, label='Specificity (Normal)', color='#3498db', edgecolor='black')\n",
                "ax.set_ylabel('Score', fontweight='bold', fontsize=12)\n",
                "ax.set_title('Sensitivity vs Specificity by Model (Ensemble)', fontsize=14, fontweight='bold')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(df['Model'], fontsize=11)\n",
                "ax.set_ylim(0.5, 1.05)\n",
                "ax.legend(loc='lower right', fontsize=11)\n",
                "ax.grid(axis='y', alpha=0.3)\n",
                "for bar in bars1:\n",
                "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
                "for bar in bars2:\n",
                "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{bar.get_height():.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('comparison_sens_spec.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Radar Chart - Overall Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from math import pi\n",
                "categories = ['Accuracy', 'AUC', 'F1-Score', 'Sensitivity', 'Specificity']\n",
                "N = len(categories)\n",
                "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
                "angles += angles[:1]\n",
                "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
                "colors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6', '#f39c12']\n",
                "for i, (_, row) in enumerate(df.iterrows()):\n",
                "    values = [row['Test Accuracy'], row['Test AUC'], row['Test F1'], row['Test Sensitivity'], row['Test Specificity']]\n",
                "    values += values[:1]\n",
                "    ax.plot(angles, values, 'o-', linewidth=2, label=row['Model'], color=colors[i])\n",
                "    ax.fill(angles, values, alpha=0.15, color=colors[i])\n",
                "ax.set_xticks(angles[:-1])\n",
                "ax.set_xticklabels(categories, fontsize=12, fontweight='bold')\n",
                "ax.set_ylim(0.6, 1.0)\n",
                "ax.set_title('Model Performance Radar Chart (Ensemble)', fontsize=16, fontweight='bold', y=1.1)\n",
                "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=11)\n",
                "plt.tight_layout()\n",
                "plt.savefig('comparison_radar_chart.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Time Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "colors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6', '#f39c12']\n",
                "bars = ax.barh(df['Model'], df['Training Time (min)'], color=colors, edgecolor='black')\n",
                "ax.set_xlabel('Training Time (minutes)', fontweight='bold', fontsize=12)\n",
                "ax.set_title('Total Training Time Comparison (3-Fold CV)', fontsize=14, fontweight='bold')\n",
                "ax.grid(axis='x', alpha=0.3)\n",
                "for bar, val in zip(bars, df['Training Time (min)']):\n",
                "    ax.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2, f'{val:.1f} min', va='center', fontsize=11, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('comparison_training_time.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Heatmap - All Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "metrics_cols = ['Test Accuracy', 'Test AUC', 'Test F1', 'Test Sensitivity', 'Test Specificity']\n",
                "heatmap_data = df[metrics_cols].copy()\n",
                "heatmap_data.index = df['Model']\n",
                "plt.figure(figsize=(12, 6))\n",
                "sns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='RdYlGn', linewidths=2, linecolor='white', vmin=0.6, vmax=1.0, annot_kws={'size': 12, 'weight': 'bold'})\n",
                "plt.title('Performance Heatmap - All Models (Ensemble)', fontsize=16, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('comparison_heatmap.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Courbes ROC Comparatives - Tous les Modeles"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "from sklearn.metrics import roc_curve, auc\n",
                "from tensorflow.keras.applications import EfficientNetB0, ResNet50\n",
                "from tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preprocess\n",
                "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
                "\n",
                "IMG_SIZE, BATCH_SIZE = 128, 32\n",
                "CLASS_MAP = {'NORMAL': 0, 'PNEUMONIA': 1}\n",
                "\n",
                "def load_test_data():\n",
                "    paths, labels = [], []\n",
                "    for cn in ['NORMAL', 'PNEUMONIA']:\n",
                "        cd = Path('chest_xray/test') / cn\n",
                "        for p in cd.glob('*'):\n",
                "            if p.suffix.lower() in ['.jpeg','.jpg','.png']:\n",
                "                paths.append(str(p)); labels.append(CLASS_MAP[cn])\n",
                "    return np.array(paths), np.array(labels)\n",
                "\n",
                "test_paths, y_true = load_test_data()\n",
                "print(f'Test: {len(y_true)} images (Normal: {sum(y_true==0)}, Pneumonia: {sum(y_true==1)})')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset creation functions for each preprocessing type\n",
                "def create_dataset_simple(paths, labels):  # CNN Base, VGG16, DenseNet121 use /255\n",
                "    def load_img(p, l):\n",
                "        img = tf.io.read_file(p)\n",
                "        img = tf.image.decode_jpeg(img, channels=3)\n",
                "        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
                "        return tf.cast(img, tf.float32) / 255.0, l\n",
                "    return tf.data.Dataset.from_tensor_slices((paths, labels)).map(load_img).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "def create_dataset_resnet(paths, labels):  # ResNet50 uses resnet preprocess_input\n",
                "    def load_img(p, l):\n",
                "        img = tf.io.read_file(p)\n",
                "        img = tf.image.decode_jpeg(img, channels=3)\n",
                "        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
                "        return resnet_preprocess(img), l\n",
                "    return tf.data.Dataset.from_tensor_slices((paths, labels)).map(load_img).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "def create_dataset_effnet(paths, labels):  # EfficientNet uses effnet preprocess_input\n",
                "    def load_img(p, l):\n",
                "        img = tf.io.read_file(p)\n",
                "        img = tf.image.decode_jpeg(img, channels=3)\n",
                "        img = tf.image.resize(img, [IMG_SIZE, IMG_SIZE])\n",
                "        return effnet_preprocess(tf.cast(img, tf.float32)), l\n",
                "    return tf.data.Dataset.from_tensor_slices((paths, labels)).map(load_img).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
                "\n",
                "# EfficientNet architecture (exactly as original)\n",
                "def create_efficientnet():\n",
                "    base = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
                "    base.trainable = False\n",
                "    inp = keras.Input(shape=(IMG_SIZE,IMG_SIZE,3))\n",
                "    x = base(inp, training=False)\n",
                "    x = layers.GlobalAveragePooling2D()(x)\n",
                "    x = layers.Dense(64)(x)\n",
                "    x = layers.BatchNormalization()(x)\n",
                "    x = layers.Activation('relu')(x)\n",
                "    x = layers.Dropout(0.3)(x)\n",
                "    out = layers.Dense(1, activation='sigmoid', dtype='float32')(x)\n",
                "    return keras.Model(inp, out)\n",
                "\n",
                "print('Dataset functions defined.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_configs = [\n",
                "    {'name': 'CNN Base', 'files': ['model_1_cnn_fold_1.keras', 'model_1_cnn_fold_2.keras', 'model_1_cnn_fold_3.keras'], 'color': '#e74c3c', 'type': 'keras', 'ds': 'simple'},\n",
                "    {'name': 'VGG16', 'files': ['model_2_vgg16_fold_1.keras', 'model_2_vgg16_fold_2.keras', 'model_2_vgg16_fold_3.keras'], 'color': '#3498db', 'type': 'keras', 'ds': 'simple'},\n",
                "    {'name': 'ResNet50', 'files': ['model_3_resnet50_fold_1.keras', 'model_3_resnet50_fold_2.keras', 'model_3_resnet50_fold_3.keras'], 'color': '#2ecc71', 'type': 'keras', 'ds': 'resnet'},\n",
                "    {'name': 'EfficientNet-B0', 'files': ['model_4_efficientnet_fold_1_weights.h5', 'model_4_efficientnet_fold_2_weights.h5', 'model_4_efficientnet_fold_3_weights.h5'], 'color': '#9b59b6', 'type': 'effnet', 'ds': 'effnet'},\n",
                "    {'name': 'DenseNet121', 'files': ['model_5_densenet121_fold_1.keras', 'model_5_densenet121_fold_2.keras', 'model_5_densenet121_fold_3.keras'], 'color': '#f39c12', 'type': 'keras', 'ds': 'simple'}\n",
                "]\n",
                "\n",
                "# Create datasets once for each preprocessing type\n",
                "ds_simple = create_dataset_simple(test_paths, y_true)\n",
                "ds_resnet = create_dataset_resnet(test_paths, y_true)\n",
                "ds_effnet = create_dataset_effnet(test_paths, y_true)\n",
                "\n",
                "roc_data = {}\n",
                "for config in model_configs:\n",
                "    print(f\"\\n{'='*50}\\n{config['name']}\")\n",
                "    fold_preds = []\n",
                "    \n",
                "    # Select correct dataset\n",
                "    if config['ds'] == 'resnet':\n",
                "        ds = ds_resnet\n",
                "    elif config['ds'] == 'effnet':\n",
                "        ds = ds_effnet\n",
                "    else:\n",
                "        ds = ds_simple\n",
                "    \n",
                "    for fold_file in config['files']:\n",
                "        if not os.path.exists(fold_file): print(f\"  Missing: {fold_file}\"); continue\n",
                "        try:\n",
                "            tf.keras.backend.clear_session()\n",
                "            if config['type'] == 'effnet':\n",
                "                model = create_efficientnet()\n",
                "                model.load_weights(fold_file)\n",
                "            else:\n",
                "                model = keras.models.load_model(fold_file)\n",
                "            preds = model.predict(ds, verbose=0).flatten()\n",
                "            fold_preds.append(preds)\n",
                "            print(f\"  Loaded: {fold_file}\")\n",
                "        except Exception as e:\n",
                "            print(f\"  ERROR: {e}\")\n",
                "    \n",
                "    if fold_preds:\n",
                "        y_pred = np.mean(fold_preds, axis=0)\n",
                "        fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
                "        roc_auc = auc(fpr, tpr)\n",
                "        if roc_auc < 0.5:\n",
                "            print(f\"  Inverting predictions (AUC was {roc_auc:.4f})\")\n",
                "            y_pred = 1 - y_pred\n",
                "            fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
                "            roc_auc = auc(fpr, tpr)\n",
                "        roc_data[config['name']] = {'fpr': fpr, 'tpr': tpr, 'auc': roc_auc, 'color': config['color']}\n",
                "        print(f\"  AUC: {roc_auc:.4f}\")\n",
                "\n",
                "print(f\"\\nLoaded {len(roc_data)} models for ROC\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(10, 8))\n",
                "for name, data in roc_data.items():\n",
                "    plt.plot(data['fpr'], data['tpr'], color=data['color'], linewidth=2.5, label=f\"{name} (AUC = {data['auc']:.4f})\")\n",
                "plt.plot([0, 1], [0, 1], 'k--', linewidth=1.5, label='Random Classifier')\n",
                "plt.xlim([0, 1]); plt.ylim([0, 1.05])\n",
                "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12, fontweight='bold')\n",
                "plt.ylabel('True Positive Rate (Sensitivity)', fontsize=12, fontweight='bold')\n",
                "plt.title('Comparative ROC Curves - All 5 Models (Ensemble)', fontsize=14, fontweight='bold')\n",
                "plt.legend(loc='lower right', fontsize=11)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.tight_layout()\n",
                "plt.savefig('comparison_roc_curves.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print('ROC curves saved: comparison_roc_curves.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. CV vs Test Performance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
                "metrics_pairs = [('CV Accuracy', 'Test Accuracy'), ('CV AUC', 'Test AUC'), ('CV F1', 'Test F1')]\n",
                "for ax, (cv_m, test_m) in zip(axes, metrics_pairs):\n",
                "    x = np.arange(len(df))\n",
                "    width = 0.35\n",
                "    ax.bar(x - width/2, df[cv_m], width, label='CV', color='#3498db', edgecolor='black')\n",
                "    ax.bar(x + width/2, df[test_m], width, label='Test', color='#e74c3c', edgecolor='black')\n",
                "    ax.set_ylabel(cv_m.replace('CV ', ''), fontweight='bold')\n",
                "    ax.set_title(f'{cv_m.replace(\"CV \", \"\")} : CV vs Test', fontweight='bold')\n",
                "    ax.set_xticks(x)\n",
                "    ax.set_xticklabels(df['Model'], rotation=45, ha='right')\n",
                "    ax.set_ylim(0.8, 1.02)\n",
                "    ax.legend(loc='lower right')\n",
                "    ax.grid(axis='y', alpha=0.3)\n",
                "plt.suptitle('Cross-Validation vs Test Set Performance', fontsize=16, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig('comparison_cv_vs_test.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Best Model Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('\\n' + '='*70)\n",
                "print('BEST MODEL FOR EACH METRIC')\n",
                "print('='*70)\n",
                "metrics_to_check = {'Test Accuracy': 'max', 'Test AUC': 'max', 'Test F1': 'max', 'Test Sensitivity': 'max', 'Test Specificity': 'max', 'Training Time (min)': 'min'}\n",
                "for metric, op in metrics_to_check.items():\n",
                "    idx = df[metric].idxmin() if op == 'min' else df[metric].idxmax()\n",
                "    print(f\"{metric:25} : {df.loc[idx, 'Model']:15} ({df.loc[idx, metric]:.4f})\")\n",
                "print('='*70)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('\\n' + '='*70)\n",
                "print('OVERALL RANKING (Weighted Score)')\n",
                "print('='*70)\n",
                "print('Weights: AUC=0.25, F1=0.25, Sensitivity=0.30, Specificity=0.20')\n",
                "df['Weighted Score'] = 0.25*df['Test AUC'] + 0.25*df['Test F1'] + 0.30*df['Test Sensitivity'] + 0.20*df['Test Specificity']\n",
                "df_ranked = df.sort_values('Weighted Score', ascending=False).reset_index(drop=True)\n",
                "df_ranked['Rank'] = range(1, len(df_ranked)+1)\n",
                "for _, row in df_ranked.iterrows():\n",
                "    print(f\"{row['Rank']}. {row['Model']:20} - Weighted Score: {row['Weighted Score']:.4f}\")\n",
                "print('='*70)\n",
                "print(f\"\\n>>> BEST OVERALL MODEL: {df_ranked.iloc[0]['Model']} <<<\")\n",
                "print(f\">>> Weighted Score: {df_ranked.iloc[0]['Weighted Score']:.4f} <<<\")\n",
                "print('='*70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Final Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('\\n' + '='*100)\n",
                "print('FINAL SUMMARY TABLE (ENSEMBLE PREDICTIONS)')\n",
                "print('='*100)\n",
                "summary_df = df_ranked[['Rank', 'Model', 'Test Accuracy', 'Test AUC', 'Test F1', 'Test Sensitivity', 'Test Specificity', 'Training Time (min)', 'Weighted Score']].copy()\n",
                "for col in summary_df.columns:\n",
                "    if col not in ['Model', 'Rank']:\n",
                "        summary_df[col] = summary_df[col].round(4)\n",
                "print(summary_df.to_string(index=False))\n",
                "print('='*100)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "comparison_output = {\n",
                "    'ranking': df_ranked[['Rank', 'Model', 'Weighted Score']].to_dict('records'),\n",
                "    'best_model': df_ranked.iloc[0]['Model'],\n",
                "    'best_score': float(df_ranked.iloc[0]['Weighted Score'])\n",
                "}\n",
                "with open('models_comparison_ensemble_final.json', 'w') as f:\n",
                "    json.dump(comparison_output, f, indent=2)\n",
                "print('Saved: models_comparison_ensemble_final.json')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Strengths & Weaknesses Table"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import display, HTML\n",
                "\n",
                "def get_val(model, metric):\n",
                "    row = df[df['Model'] == model]\n",
                "    return row[metric].values[0] if len(row) > 0 else 0\n",
                "\n",
                "data_sw = {\n",
                "    'Modele': ['CNN Base', 'VGG16', 'ResNet50', 'EfficientNet-B0', 'DenseNet121'],\n",
                "    'Forces': [\n",
                "        f\"Simple, rapide ({get_val('CNN Base', 'Training Time (min)'):.1f} min)\",\n",
                "        f\"Sensibilite elevee ({get_val('VGG16', 'Test Sensitivity')*100:.1f}%)\",\n",
                "        f\"Meilleur equilibre, Accuracy ({get_val('ResNet50', 'Test Accuracy')*100:.1f}%)\",\n",
                "        f\"Architecture efficiente, AUC ({get_val('EfficientNet-B0', 'Test AUC'):.4f})\",\n",
                "        f\"Dense connections, Specificite ({get_val('DenseNet121', 'Test Specificity')*100:.1f}%)\"\n",
                "    ],\n",
                "    'Faiblesses': [\n",
                "        f\"Specificite faible ({get_val('CNN Base', 'Test Specificity')*100:.1f}%)\",\n",
                "        f\"Specificite moderee ({get_val('VGG16', 'Test Specificity')*100:.1f}%)\",\n",
                "        f\"Modele volumineux, Specificite ({get_val('ResNet50', 'Test Specificity')*100:.1f}%)\",\n",
                "        f\"Sensibilite moderee ({get_val('EfficientNet-B0', 'Test Sensitivity')*100:.1f}%)\",\n",
                "        f\"Temps long ({get_val('DenseNet121', 'Training Time (min)'):.1f} min)\"\n",
                "    ],\n",
                "    'Usage': ['Prototypage', 'Screening', 'Diagnostic', 'Deploiement leger', 'Recherche'],\n",
                "    'Score': [get_val(m, 'Weighted Score') if 'Weighted Score' in df.columns else 0 for m in ['CNN Base', 'VGG16', 'ResNet50', 'EfficientNet-B0', 'DenseNet121']]\n",
                "}\n",
                "df_sw = pd.DataFrame(data_sw)\n",
                "best_model = df_ranked.iloc[0]['Model']\n",
                "second_model = df_ranked.iloc[1]['Model']\n",
                "\n",
                "html = '<style>.tbl{border-collapse:collapse;width:100%;font-size:14px;color:#000}.tbl th{background:#2c3e50;color:white;padding:12px}.tbl td{padding:12px;border:1px solid #ddd}.tbl tr:nth-child(even){background:#f8f9fa}.best{background:#d4edda!important;border-left:4px solid #1e7e34}.second{background:#fff3cd!important;border-left:4px solid #c69500}</style><table class=\"tbl\"><tr><th>Modele</th><th>Forces</th><th>Faiblesses</th><th>Usage</th><th>Score</th></tr>'\n",
                "for i, row in df_sw.iterrows():\n",
                "    cls = 'best' if row['Modele'] == best_model else ('second' if row['Modele'] == second_model else '')\n",
                "    html += f'<tr class=\"{cls}\"><td><b>{row[\"Modele\"]}</b></td><td>{row[\"Forces\"]}</td><td>{row[\"Faiblesses\"]}</td><td>{row[\"Usage\"]}</td><td><b>{row[\"Score\"]:.4f}</b></td></tr>'\n",
                "html += '</table>'\n",
                "display(HTML(html))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Matrices de Confusion Globales"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_normal, test_pneumonia = 234, 390\n",
                "models_cm = {row['Model']: {'sensitivity': row['Test Sensitivity'], 'specificity': row['Test Specificity']} for _, row in df.iterrows()}\n",
                "cmaps = ['Blues', 'Oranges', 'Greens', 'Reds', 'Purples']\n",
                "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']\n",
                "\n",
                "fig, axes = plt.subplots(2, 3, figsize=(16, 11))\n",
                "axes = axes.flatten()\n",
                "for idx, (model, metrics) in enumerate(models_cm.items()):\n",
                "    TP = int(test_pneumonia * metrics['sensitivity'])\n",
                "    TN = int(test_normal * metrics['specificity'])\n",
                "    cm = np.array([[TN, test_normal-TN], [test_pneumonia-TP, TP]])\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmaps[idx], ax=axes[idx], xticklabels=['Normal', 'Pneumonie'], yticklabels=['Normal', 'Pneumonie'], annot_kws={'size': 16, 'weight': 'bold'}, linewidths=2)\n",
                "    axes[idx].set_xlabel('Prediction', fontweight='bold')\n",
                "    axes[idx].set_ylabel('Reel', fontweight='bold')\n",
                "    axes[idx].set_title(model, fontsize=13, fontweight='bold', color=colors[idx])\n",
                "axes[5].axis('off')\n",
                "axes[5].text(0.5, 0.5, f'MEILLEUR:\\n{df_ranked.iloc[0][\"Model\"]}', ha='center', va='center', fontsize=14, fontweight='bold', bbox=dict(boxstyle='round', facecolor='#d4edda', edgecolor='#1e7e34', linewidth=2), transform=axes[5].transAxes)\n",
                "plt.suptitle('Matrices de Confusion (Ensemble)', fontsize=16, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig('global_confusion_matrices.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 12. Analyse Textuelle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from IPython.display import display, Markdown\n",
                "best = df_ranked.iloc[0]\n",
                "second = df_ranked.iloc[1]\n",
                "analysis = f'''# POURQUOI {best['Model'].upper()} GAGNE ?\\n\\n| Rang | Modele | Score |\\n|------|--------|-------|\\n'''\n",
                "for _, row in df_ranked.iterrows():\n",
                "    bold = '**' if row['Rank'] == 1 else ''\n",
                "    analysis += f\"| {bold}{row['Rank']}{bold} | {bold}{row['Model']}{bold} | {bold}{row['Weighted Score']:.4f}{bold} |\\n\"\n",
                "analysis += f'''\\n## Metriques Cles de {best['Model']}\\n- **Accuracy**: {best['Test Accuracy']*100:.2f}%\\n- **AUC**: {best['Test AUC']:.4f}\\n- **F1**: {best['Test F1']*100:.2f}%\\n- **Sensibilite**: {best['Test Sensitivity']*100:.2f}%\\n- **Specificite**: {best['Test Specificity']*100:.2f}%\\n\\n## Conclusion\\n**{best['Model']}** offre le meilleur equilibre entre detection des pneumonies et reduction des faux positifs.'''\n",
                "display(Markdown(analysis))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 13. Recapitulatif Final"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "best_sens = df.loc[df['Test Sensitivity'].idxmax()]\n",
                "best_spec = df.loc[df['Test Specificity'].idxmax()]\n",
                "best_time = df.loc[df['Training Time (min)'].idxmin()]\n",
                "print('='*80)\n",
                "print('CHOIX DU MODELE SELON L\\'OBJECTIF')\n",
                "print('='*80)\n",
                "print(f'''\\n| Objectif             | Modele recommande                              |\\n|----------------------|------------------------------------------------|\\n| Diagnostic clinique  | {df_ranked.iloc[0]['Model']} - meilleur equilibre         |\\n| Screening massif     | {best_sens['Model']} - sensibilite max ({best_sens['Test Sensitivity']*100:.1f}%)       |\\n| Eviter faux positifs | {best_spec['Model']} - specificite max ({best_spec['Test Specificity']*100:.1f}%)  |\\n| Prototypage rapide   | {best_time['Model']} - plus rapide ({best_time['Training Time (min)']:.1f} min)        |\\n''')\n",
                "print('Legende: Vert = meilleur, Jaune = second')\n",
                "print('='*80)\n",
                "print('\\nCOMPARISON COMPLETE!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print('\\nFigures generees:')\n",
                "for f in ['comparison_test_metrics', 'comparison_sens_spec', 'comparison_radar_chart', 'comparison_training_time', 'comparison_heatmap', 'comparison_roc_curves', 'comparison_cv_vs_test', 'global_confusion_matrices']:\n",
                "    print(f'  - {f}.png')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "tf-2.10",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.19"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
